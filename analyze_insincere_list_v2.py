import requests
import zipfile
import io
import datetime
import time
from bs4 import BeautifulSoup
from dateutil.relativedelta import relativedelta

API_KEY = "577d4d3fff35faf9705ef7383b323d98e87a84bd"

target_companies_str = """
í”¼ì—”í‹°ì— ì—ìŠ¤, ìí™”ì „ì, ì—‘ì‹œì˜¨ê·¸ë£¹, ì”¨ì”¨ì—ìŠ¤, ë„¥ìŠ¤í‹¸, íƒœê´‘ì‚°ì—…, íŒ¡ìŠ¤ì¹´ì´, ì¸í”¼ë‹ˆíŠ¸í—¬ìŠ¤ì¼€ì–´, ì—ìŠ¤ì—˜ì—ìŠ¤ë°”ì´ì˜¤, ì‹ ëŒ€ì–‘ì œì§€, ì˜¤ì—ìŠ¤í”¼, ì§€ì—ì´ì´ë…¸ë”ìŠ¤, ê´‘ë™ì œì•½, ë“œë˜ê³¤í”Œë¼ì´, ì—ì½”ë³¼íŠ¸, ì—˜ë¦¬ë¹„ì ¼, ë™ì›ê°œë°œ, ì…€ë£¨ë©”ë“œ, ì¼€ì´ì”¨ì”¨, ë™ì„±ì œì•½, í‹°ì—ìŠ¤ë„¥ìŠ¤ì  , ìì´ê¸€, ë²”ì–‘ê±´ì˜, ì•„ì´ì—ì´, ì½”ì•„ìŠ¤, EMB, í•œêµ­ë¯¸ë¼í´í”¼í”Œì‚¬, ì—ì½”í”Œë¼ìŠ¤í‹±, KSì¸ë”ìŠ¤íŠ¸ë¦¬, ì˜í’, ì•„ì´ì—ìŠ¤ë™ì„œ, í•˜ë‚˜ë§ˆì´í¬ë¡ , ìº”ë²„ìŠ¤ì—”, í¬ìŠ¤ì½”DX, ì„œìš¸ë„ì‹œê°€ìŠ¤, íœ´ë¨¼í…Œí¬ë†€ë¡œì§€, í¬ë ˆì˜¤ì—ìŠ¤ì§€, ë“œë˜ê³¤í”Œë¼ì´, ì•„ë¯¸ì½”ì  , ë”í…Œí¬ë†€ë¡œì§€, ì‹œìŠ¤ì›, ì˜¬ë¦¬íŒ¨ìŠ¤, ì¹´ì´ë…¸ìŠ¤ë©”ë“œ, ì„¸ê²½í•˜ì´í…Œí¬, ë‚˜ë…¸ì‹¤ë¦¬ì¹¸ì²¨ë‹¨ì†Œì¬, ì§„ì‹œìŠ¤í…œ, ì‹ ë¼ì  , í•œêµ­ìœ ë‹ˆì˜¨ì œì•½, ë§Œí˜¸ì œê°•, íƒœê´‘ì‚°ì—…, ì„¸ì¤‘, ì½”ë‚˜ì†”, í•œì§„, í•œì„±ê¸°ì—…, ë™ì„±ì œì•½, ë‚˜ë…¸ì‹¤ë¦¬ì¹¸ì²¨ë‹¨ì†Œì¬, ì˜ì¹´, ì”¨ì—”í”ŒëŸ¬ìŠ¤, ì§„ì›ìƒëª…ê³¼í•™, í…Œë¼ì‚¬ì´ì–¸ìŠ¤, ì½œë§ˆë¹„ì•¤ì—ì´ì¹˜, íŒŒë¼í…, ì”¨ì”¨ì—ìŠ¤, ì˜¤ê±´ì—ì½”í…, ë©¤ë ˆì´ë¹„í‹°, ì½”ì•„ìŠ¤, ì„œì§„ì‹œìŠ¤í…œ, ì¡°ì„ ë‚´í™”, ì‹ í’ì œì•½, ë‹¤ì˜¬íˆ¬ìì¦ê¶Œ, ì§€ë”ë¸”ìœ ë°”ì´í…, ì˜¤ê±´ì—ì½”í…, ì—ì´ì „íŠ¸AI, GRT, ì˜¬ë¦¬íŒ¨ìŠ¤, í…Œë¼ì‚¬ì´ì–¸ìŠ¤, ëŒ€ì‚°F&B, ì¸íŠ¸ë¡œë©”ë”•, ì•„ìŠ¤íƒ€, ë©”ë””ì•™ìŠ¤, ë¡œë³´ì“°ë¦¬ì—ì´ì•„ì´, ì´ë¯¸ì§€ìŠ¤, THE E&M, ì¸í¬ë ˆë”ë¸”ë²„ì¦ˆ, ì„¸í† í”¼ì•„, ì—ìŠ¤ì•¤ë”ë¸”ë¥˜, DHì˜¤í† ë„¥ìŠ¤, í•œì„¸ì˜ˆìŠ¤24í™€ë”©ìŠ¤, í‹°ì—ìŠ¤ë„¥ìŠ¤ì  , ë²”ì–‘ê±´ì˜, HSíš¨ì„±ì²¨ë‹¨ì†Œì¬, í•œêµ­ìœ ë‹ˆì˜¨ì œì•½, ì œì´ìŠ¤ì½”í™€ë”©ìŠ¤, í’€ë¬´ì›, ì œì´ì˜¤, ê´‘ëª…ì „ê¸°, ì•Œì—í”„ì„¸ë¯¸, ê¸ˆí˜¸ì „ê¸°, OCI, ë‹¤ë³´ë§í¬, ê¸ˆì–‘, ë‚˜ë¼ì†Œí”„íŠ¸, ë°”ì´ì˜¨, ì´ìˆ˜í˜íƒ€ì‹œìŠ¤, ì˜¤í…, ì½”ì˜¤ë¡±ìƒëª…ê³¼í•™, ë””ì™€ì´ë””, ì œì´ì—ìŠ¤ë§í¬, STX, ë¹„ì¸ ë¡œì‹œìŠ¤, ì‚¼ì˜ì´ì—”ì”¨, ì œì´ì¼€ì´ì‹œëƒ…ìŠ¤, ê²½ì¸ì–‘í–‰, ìì´ê¸€, í…Œí¬íŠ¸ëœìŠ¤, í•œìš¸ì•¤ì œì£¼, ì‹¸ì´í† ì  , ëŒ€ì–‘ê¸ˆì†, ê³ ë ¤ì•„ì—°, ë…¸ë¸”ì— ì•¤ë¹„, ì´ì˜¤í”Œë¡œìš°, KSì¸ë”ìŠ¤íŠ¸ë¦¬, í•œì„ ì—”ì§€ë‹ˆì–´ë§, ì•Œë©•, ì†Œí”„íŠ¸ì„¼, ë‚˜ë…¸ì‹¤ë¦¬ì¹¸ì²¨ë‹¨ì†Œì¬, í”„ë¡œë¸Œì‡, ì…€í”¼ê¸€ë¡œë²Œ, ì˜µíŠ¸ë¡ í…, êµ­ì œì•½í’ˆ
"""

target_set = {name.strip() for name in target_companies_str.replace('\n', ',').split(',') if name.strip()}

def get_insincere_list_chunked():
    print(f"ğŸ¯ Target Companies: {len(target_set)} unique corps")
    url = "https://opendart.fss.or.kr/api/list.json"
    
    all_items = []
    
    # Iterate last 1 year in 3-month chunks
    end_date = datetime.datetime.now()
    start_date = end_date - relativedelta(years=1)
    
    current_start = start_date
    while current_start < end_date:
        current_end = current_start + relativedelta(months=3)
        if current_end > end_date: current_end = end_date
        
        print(f"â³ Scanning: {current_start.strftime('%Y-%m-%d')} ~ {current_end.strftime('%Y-%m-%d')} ...")
        
        for page in range(1, 5): # Check up to 5 pages per chunk
            params = {
                "crtfc_key": API_KEY,
                "bgn_de": current_start.strftime("%Y%m%d"),
                "end_de": current_end.strftime("%Y%m%d"),
                "page_no": page,
                "page_count": 100
            }
            try:
                resp = requests.get(url, params=params).json()
                if resp.get('status') == '000' and 'list' in resp:
                    for item in resp['list']:
                        if "ë¶ˆì„±ì‹¤" in item['report_nm']:
                            all_items.append(item)
                    if len(resp['list']) < 100: break # Last page
                else:
                    break
            except Exception as e:
                print(f"âŒ Error: {e}")
                break
            time.sleep(0.1)
            
        current_start = current_end + relativedelta(days=1)

    print(f"âœ… Total Insincere Reports Found: {len(all_items)}")
    
    # Filter
    matches = [item for item in all_items if item['corp_name'] in target_set]
    print(f"ğŸ”” Matches in your list: {len(matches)}")
    return matches

def extract_violation_detail(rcept_no):
    url = "https://opendart.fss.or.kr/api/document.xml"
    params = {"crtfc_key": API_KEY, "rcept_no": rcept_no}
    try:
        resp = requests.get(url, params=params)
        with zipfile.ZipFile(io.BytesIO(resp.content)) as z:
            target_file = next((f for f in z.namelist() if f.endswith('.xml') or f.endswith('.html')), None)
            if not target_file: return "No XML"
            
            # Use lxml for better table parsing if available, else html.parser
            soup = BeautifulSoup(z.read(target_file), 'html.parser')
            
            # Strategy: Simply find where the 'designation content' starts
            # Usually inside a table or section titled "2. ë¶ˆì„±ì‹¤ê³µì‹œë²•ì¸ ì§€ì •ë‚´ìš©"
            
            full_text = soup.get_text('\n')
            lines = [line.strip() for line in full_text.split('\n') if line.strip()]
            
            found_idx = -1
            keywords = ["ì§€ì •ë‚´ìš©", "ìœ„ë°˜ë‚´ìš©", "ê³µì‹œìœ„ë°˜ ë‚´ìš©", "ë¶ˆì„±ì‹¤ê³µì‹œë²•ì¸ ì§€ì •"]
            
            # 1. Find the starting line index
            for i, line in enumerate(lines):
                 if any(k in line for k in keywords) and len(line) < 50:
                     found_idx = i
                     break
            
            extracted = []
            if found_idx != -1:
                # Capture next 15 lines after the header
                # Filter out pure numbers or dates to get text
                count = 0
                for line in lines[found_idx+1:]:
                    if "3." in line or "ë¶€ê³¼ë²Œì " in line: # Stop condition
                         extracted.append(line)
                         break 
                    extracted.append(line)
                    count += 1
                    if count > 10: break
            else:
                 # Fallback: Search for sentences containing specific patterns
                 for line in lines:
                     if "ê³µì‹œë²ˆë³µ" in line or "ê³µì‹œë¶ˆì´í–‰" in line or "ë³€ê²½" in line:
                         if len(line) > 10 and len(line) < 200:
                            extracted.append(line)

            return "\n".join(extracted) if extracted else "ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë¬¸ì„œ êµ¬ì¡°ê°€ íŠ¹ì´í•¨)"

    except Exception as e:
        return f"Error: {e}"

def main():
    matches = get_insincere_list_chunked()
    
    # Create Markdown Report
    report_content = f"# ğŸš¨ ë¶ˆì„±ì‹¤ê³µì‹œë²•ì¸ ë¶„ì„ ë¦¬í¬íŠ¸\nì‘ì„±ì¼: {datetime.datetime.now().strftime('%Y-%m-%d')}\n\n"
    
    if not matches:
        report_content += "## âœ… ë¶„ì„ ê²°ê³¼: í•´ë‹¹ ë¦¬ìŠ¤íŠ¸ ë‚´ ê¸°ì—… ì¤‘ ìµœê·¼ 1ë…„ê°„ ë¶ˆì„±ì‹¤ê³µì‹œ ë‚´ì—­ ì—†ìŒ.\n"
    else:
        for item in matches:
            report_content += f"## âš ï¸ {item['corp_name']}\n"
            report_content += f"- **ê³µì‹œì¼**: {item['rcept_dt']}\n"
            report_content += f"- **ê³µì‹œëª…**: {item['report_nm']}\n"
            
            print(f"   > Fetching details for {item['corp_name']}...")
            detail = extract_violation_detail(item['rcept_no'])
            report_content += f"- **ìœ„ë°˜ìƒì„¸**: \n```\n{detail}\n```\n\n---\n"
            
    with open("insincere_report_final.md", "w", encoding="utf-8") as f:
        f.write(report_content)
    print("\nğŸ‰ Report Saved: insincere_report_final.md")

if __name__ == "__main__":
    main()
