import requests
import zipfile
import io
import datetime
import time
from bs4 import BeautifulSoup

API_KEY = "577d4d3fff35faf9705ef7383b323d98e87a84bd"

# User provided list (cleaned up)
target_companies_str = """
í”¼ì—”í‹°ì— ì—ìŠ¤, ìí™”ì „ì, ì—‘ì‹œì˜¨ê·¸ë£¹, ì”¨ì”¨ì—ìŠ¤, ë„¥ìŠ¤í‹¸, íƒœê´‘ì‚°ì—…, íŒ¡ìŠ¤ì¹´ì´, ì¸í”¼ë‹ˆíŠ¸í—¬ìŠ¤ì¼€ì–´, ì—ìŠ¤ì—˜ì—ìŠ¤ë°”ì´ì˜¤, ì‹ ëŒ€ì–‘ì œì§€, ì˜¤ì—ìŠ¤í”¼, ì§€ì—ì´ì´ë…¸ë”ìŠ¤, ê´‘ë™ì œì•½, ë“œë˜ê³¤í”Œë¼ì´, ì—ì½”ë³¼íŠ¸, ì—˜ë¦¬ë¹„ì ¼, ë™ì›ê°œë°œ, ì…€ë£¨ë©”ë“œ, ì¼€ì´ì”¨ì”¨, ë™ì„±ì œì•½, í‹°ì—ìŠ¤ë„¥ìŠ¤ì  , ìì´ê¸€, ë²”ì–‘ê±´ì˜, ì•„ì´ì—ì´, ì½”ì•„ìŠ¤, EMB, í•œêµ­ë¯¸ë¼í´í”¼í”Œì‚¬, ì—ì½”í”Œë¼ìŠ¤í‹±, KSì¸ë”ìŠ¤íŠ¸ë¦¬, ì˜í’, ì•„ì´ì—ìŠ¤ë™ì„œ, í•˜ë‚˜ë§ˆì´í¬ë¡ , ìº”ë²„ìŠ¤ì—”, í¬ìŠ¤ì½”DX, ì„œìš¸ë„ì‹œê°€ìŠ¤, íœ´ë¨¼í…Œí¬ë†€ë¡œì§€, í¬ë ˆì˜¤ì—ìŠ¤ì§€, ë“œë˜ê³¤í”Œë¼ì´, ì•„ë¯¸ì½”ì  , ë”í…Œí¬ë†€ë¡œì§€, ì‹œìŠ¤ì›, ì˜¬ë¦¬íŒ¨ìŠ¤, ì¹´ì´ë…¸ìŠ¤ë©”ë“œ, ì„¸ê²½í•˜ì´í…Œí¬, ë‚˜ë…¸ì‹¤ë¦¬ì¹¸ì²¨ë‹¨ì†Œì¬, ì§„ì‹œìŠ¤í…œ, ì‹ ë¼ì  , í•œêµ­ìœ ë‹ˆì˜¨ì œì•½, ë§Œí˜¸ì œê°•, íƒœê´‘ì‚°ì—…, ì„¸ì¤‘, ì½”ë‚˜ì†”, í•œì§„, í•œì„±ê¸°ì—…, ë™ì„±ì œì•½, ë‚˜ë…¸ì‹¤ë¦¬ì¹¸ì²¨ë‹¨ì†Œì¬, ì˜ì¹´, ì”¨ì—”í”ŒëŸ¬ìŠ¤, ì§„ì›ìƒëª…ê³¼í•™, í…Œë¼ì‚¬ì´ì–¸ìŠ¤, ì½œë§ˆë¹„ì•¤ì—ì´ì¹˜, íŒŒë¼í…, ì”¨ì”¨ì—ìŠ¤, ì˜¤ê±´ì—ì½”í…, ë©¤ë ˆì´ë¹„í‹°, ì½”ì•„ìŠ¤, ì„œì§„ì‹œìŠ¤í…œ, ì¡°ì„ ë‚´í™”, ì‹ í’ì œì•½, ë‹¤ì˜¬íˆ¬ìì¦ê¶Œ, ì§€ë”ë¸”ìœ ë°”ì´í…, ì˜¤ê±´ì—ì½”í…, ì—ì´ì „íŠ¸AI, GRT, ì˜¬ë¦¬íŒ¨ìŠ¤, í…Œë¼ì‚¬ì´ì–¸ìŠ¤, ëŒ€ì‚°F&B, ì¸íŠ¸ë¡œë©”ë”•, ì•„ìŠ¤íƒ€, ë©”ë””ì•™ìŠ¤, ë¡œë³´ì“°ë¦¬ì—ì´ì•„ì´, ì´ë¯¸ì§€ìŠ¤, THE E&M, ì¸í¬ë ˆë”ë¸”ë²„ì¦ˆ, ì„¸í† í”¼ì•„, ì—ìŠ¤ì•¤ë”ë¸”ë¥˜, DHì˜¤í† ë„¥ìŠ¤, í•œì„¸ì˜ˆìŠ¤24í™€ë”©ìŠ¤, í‹°ì—ìŠ¤ë„¥ìŠ¤ì  , ë²”ì–‘ê±´ì˜, HSíš¨ì„±ì²¨ë‹¨ì†Œì¬, í•œêµ­ìœ ë‹ˆì˜¨ì œì•½, ì œì´ìŠ¤ì½”í™€ë”©ìŠ¤, í’€ë¬´ì›, ì œì´ì˜¤, ê´‘ëª…ì „ê¸°, ì•Œì—í”„ì„¸ë¯¸, ê¸ˆí˜¸ì „ê¸°, OCI, ë‹¤ë³´ë§í¬, ê¸ˆì–‘, ë‚˜ë¼ì†Œí”„íŠ¸, ë°”ì´ì˜¨, ì´ìˆ˜í˜íƒ€ì‹œìŠ¤, ì˜¤í…, ì½”ì˜¤ë¡±ìƒëª…ê³¼í•™, ë””ì™€ì´ë””, ì œì´ì—ìŠ¤ë§í¬, STX, ë¹„ì¸ ë¡œì‹œìŠ¤, ì‚¼ì˜ì´ì—”ì”¨, ì œì´ì¼€ì´ì‹œëƒ…ìŠ¤, ê²½ì¸ì–‘í–‰, ìì´ê¸€, í…Œí¬íŠ¸ëœìŠ¤, í•œìš¸ì•¤ì œì£¼, ì‹¸ì´í† ì  , ëŒ€ì–‘ê¸ˆì†, ê³ ë ¤ì•„ì—°, ë…¸ë¸”ì— ì•¤ë¹„, ì´ì˜¤í”Œë¡œìš°, KSì¸ë”ìŠ¤íŠ¸ë¦¬, í•œì„ ì—”ì§€ë‹ˆì–´ë§, ì•Œë©•, ì†Œí”„íŠ¸ì„¼, ë‚˜ë…¸ì‹¤ë¦¬ì¹¸ì²¨ë‹¨ì†Œì¬, í”„ë¡œë¸Œì‡, ì…€í”¼ê¸€ë¡œë²Œ, ì˜µíŠ¸ë¡ í…, êµ­ì œì•½í’ˆ
"""

# Convert to set for O(1) lookup
target_set = {name.strip() for name in target_companies_str.replace('\n', ',').split(',') if name.strip()}

def get_insincere_list():
    print(f"ğŸ¯ Target Companies: {len(target_set)} unique corps")
    print("ğŸ“¥ Searching DART for 'Insincere Disclosure' reports (Last 1 Year)...")
    
    url = "https://opendart.fss.or.kr/api/list.json"
    end_dt = datetime.datetime.now()
    start_dt = end_dt - datetime.timedelta(days=730) # 2 Years lookback
    
    all_insincere = []
    
    # Check 10 pages to be safe
    for page in range(1, 11):
        params = {
            "crtfc_key": API_KEY,
            "bgn_de": start_dt.strftime("%Y%m%d"),
            "end_de": end_dt.strftime("%Y%m%d"),
            "page_no": page,
            "page_count": 100,
            # Remove pblntf_detail_ty to search everywhere
        }
        resp = requests.get(url, params=params).json()
        if 'list' in resp:
            # DEBUG: Print first item to check
            if page == 1 and len(resp['list']) > 0:
                print(f"DEBUG: First item found -> {resp['list'][0]['corp_name']} : {resp['list'][0]['report_nm']}")

            for item in resp['list']:
                # Broaden filter: "ë¶ˆì„±ì‹¤" keyword anywhere
                # Simplify to just check everything for now to debug
                if "ë¶ˆì„±ì‹¤" in item['report_nm']:
                     all_insincere.append(item)
        else:
            print(f"DEBUG: No list in response. Status: {resp.get('status')}, Msg: {resp.get('message')}")
            break
        time.sleep(0.1) # Faster query
    
    print(f"âœ… Found {len(all_insincere)} total insincere designations.")
    
    # Filter by user list
    matches = [item for item in all_insincere if item['corp_name'] in target_set]
    print(f"ğŸ”” Matches in your list: {len(matches)}")
    return matches

def extract_violation_detail(rcept_no):
    url = "https://opendart.fss.or.kr/api/document.xml"
    params = {"crtfc_key": API_KEY, "rcept_no": rcept_no}
    
    try:
        resp = requests.get(url, params=params)
        with zipfile.ZipFile(io.BytesIO(resp.content)) as z:
            target_file = next((f for f in z.namelist() if f.endswith('.xml') or f.endswith('.html')), None)
            if not target_file: return "No XML"
            
            soup = BeautifulSoup(z.read(target_file), 'html.parser')
            text = soup.get_text('\n')
            
            # Extract relevant section
            lines = text.split('\n')
            detail = []
            capture = False
            
            # Keywords to find the reason
            start_keywords = ["ë¶ˆì„±ì‹¤ê³µì‹œë²•ì¸ ì§€ì •ë‚´ìš©", "ì§€ì •ë‚´ìš©", "ê³µì‹œìœ„ë°˜ ë‚´ìš©", "ìœ„ë°˜ë‚´ìš©"]
            end_keywords = ["ë¶€ê³¼ë²Œì ", "ê³µì‹œìœ„ë°˜ì œì¬ê¸ˆ", "3.", "ë§¤ë§¤ê±°ë˜ì •ì§€"]
            
            for line in lines:
                clean_line = line.strip()
                if not clean_line: continue
                
                # Start capture
                if not capture:
                    # Check if line contains keyword but isn't too long/garbage
                    if any(k in clean_line for k in start_keywords) and len(clean_line) < 50:
                        capture = True
                        continue
                        
                # End capture
                if capture:
                    if any(k in clean_line for k in end_keywords):
                        break
                    detail.append(clean_line)
            
            # Fallback text search if clean extract fails
            if not detail:
                for i, line in enumerate(lines):
                    if "ì‚¬ìœ " in line or "ìœ„ë°˜" in line:
                         detail.append(line)
                         if len(detail) > 3: break # Limit
            
            return " ".join(detail[:5]) if detail else "ìƒì„¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"
            
    except Exception:
        return "Download Fail"

def main():
    matches = get_insincere_list()
    
    report_lines = ["# ğŸš¨ ë¶ˆì„±ì‹¤ê³µì‹œë²•ì¸ ìœ„ë°˜ ë‚´ìš© ë¶„ì„ ê²°ê³¼", f"ë¶„ì„ì¼: {datetime.datetime.now().strftime('%Y-%m-%d')}", ""]
    
    for i, item in enumerate(matches):
        print(f"[{i+1}/{len(matches)}] Analyzing {item['corp_name']}...")
        reason = extract_violation_detail(item['rcept_no'])
        
        report_lines.append(f"### ğŸ”´ {item['corp_name']}")
        report_lines.append(f"- **ê³µì‹œì¼**: {item['rcept_dt']}")
        report_lines.append(f"- **ê³µì‹œì œëª©**: {item['report_nm']}")
        report_lines.append(f"- **ìœ„ë°˜/ì§€ì • ìƒì„¸ì‚¬ìœ **: \n  > {reason}")
        report_lines.append("---")
        
    with open("insincere_analysis.md", "w", encoding="utf-8") as f:
        f.write("\n".join(report_lines))
        
    print("\nâœ… Done! Saved to insincere_analysis.md")

if __name__ == "__main__":
    main()
